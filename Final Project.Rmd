---
title: "STAC67 Final Project Report"
author: "Nilson Gao, Vedat Goktepe, Rebecca Han, Ben Wang"
date: "2024-12-03"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

[TO DO: make this in title page as required by rubric]

```{r setup, include=FALSE, echo=FALSE}
library(dplyr)
library(tidyr)
library(leaps)
library(ggplot2)
library(gridExtra)
library(patchwork)
library(ggcorrplot)
library(car)
library(MASS)
library(lmtest)
```

# Research Context

The objective of this model is to investigate the intrinsic factors that affect Price of Cars in Serbia based off detailed car listings provided by an online marketplace where [TO DO - yap about car price significance, what question we want to answer, how this can be beneficial knowledge for consumers/dealerships/car companies/manufacturers/etc.] [For consumers it might be beneficial to know predicted prices to avoid getting overcharged]

# Exploratory Data Analysis

```{r}
# read data file, published in 2024 on
# https://www.kaggle.com/datasets/mmakarovlab/serbia-car-sales-prices?resource=download
car_price_data <- read.csv("serbia_car_sales_price_2024.csv")
validation_data <- read.csv("serbia_car_sales_price_2024.csv")

```

Before we begin investigating, we notice that there are some issues with the data. Some rows are missing values under certain variables (i.e. #2, #233, #1705, etc.), and some variables are hard to work with. Knowing a car's **year** might be less informative than knowing its age, so we made a new column containing values for $2024-\text{Year}$ called **age**. A car's **horsepower** is significant, but it's hard to use that data when it's given as two values in the format $HP\ (kW)$, so we keep only the HP metric. Additionally, some variable names are hard to work with because of length or how it might interfere with R code, such as **car_mileage, km**, so we made those easier to process as well. As for the missing values, when we analyze the significance of a variable, we'll make sure to exclude rows where values for that variable are empty.

```{r}
clean_data <- car_price_data
clean_data[clean_data == ""] <- NA
clean_data <- na.omit(clean_data)

clean_data$age <- 2024 - clean_data$year
clean_data$horsepower<-gsub(pattern = "^(\\d+) HP.*", replacement = "\\1", clean_data$horsepower)
clean_data$horsepower<-as.numeric(clean_data$horsepower)

# making the variable names easier to process
names(clean_data) <- gsub(pattern = "\\.\\..*", replacement = "", names(clean_data))
```

Now, we want to check on which variables are good predictors. For the continuous variables, we first plot scatter graphs for each variable against car price:

```{r echo=FALSE}
p1 <- ggplot(clean_data, aes(x = age, y = price)) + geom_point() +
  theme_minimal() + ggtitle("age vs price")
p2 <- ggplot(clean_data, aes(x = horsepower, y = price)) + geom_point() +
  theme_minimal() + ggtitle("horsepower vs price")
p3 <- ggplot(clean_data, aes(x = car_mileage, y = price)) + geom_point() +
  theme_minimal() + ggtitle("car_mileage vs price") # needs outlier removed
p4 <- ggplot(clean_data, aes(x = engine_capacity, y = price)) + geom_point() +
  theme_minimal() + ggtitle("engine_capacity vs price")
grid.arrange(p1,p2,p3,p4,ncol=2)
```

We notice that there are some influential points (and possibly leverage points) on the car_mileage predictor. Let's get rid of those using hat values and semistudentized residuals to detect which ones are too high:

```{r}
outlier_clean <- function(data, model) {
  #removal of points in x (leverage points)
  leverage <- hatvalues(model)
  threshold <- 2* length(coef(model)) / nrow(data)
  leverage_points <- which(leverage > threshold | leverage > 0.5)
  clean <- data[-leverage_points,]
  
  #remove outliers
  n <- nrow(data)
  threshold <- qt(1 - (0.05/(2*n)), n-2-1)
  semistudentized_residuals <- rstandard(model)
  outlier_points <- which(abs(semistudentized_residuals) > threshold)
  clean <- clean[-outlier_points,]
  
  #remove influential
  cooks_d <- cooks.distance(model)
  cooks_threshold <- 4/n
  influential_points <- which(cooks_d > cooks_threshold)
  clean <- clean[-influential_points,]
  return(clean)
}

cm_model <- lm(price ~ car_mileage, data = clean_data)
clean_cm <- outlier_clean(clean_data,cm_model)
#trim car mileage for better visibility
clean_cm2<- subset(clean_cm_data, car_mileage <= 750000)

hp_model <- lm(price ~ horsepower, data = clean_data)
clean_hp <- outlier_clean(clean_data, hp_model)

age_model <- lm(price ~ age, data = clean_data)
clean_age <- outlier_clean(clean_data, age_model)

ec_model <- lm(price ~ engine_capacity, data = clean_data)
clean_ec <- outlier_clean(clean_data, ec_model)

```

After outlier cleaning, let's check on the scatter plots again to see if these continuous variables have a significant influence on the car price:

```{r}
cm_plot <- ggplot(clean_cm, aes(x = car_mileage, y = price)) + geom_point() +
  geom_smooth(method = "lm", se = TRUE, color = "blue") + theme_minimal() + ggtitle("car_mileage vs price")
cm_plot2 <- ggplot(clean_cm2, aes(x = car_mileage, y = price)) + geom_point() + geom_smooth(method = "lm", se = TRUE, color = "blue") + theme_minimal() + ggtitle("car_mileage vs price")
hp_plot <- ggplot(clean_hp, aes(x = horsepower, y = price)) + geom_point() + geom_smooth(method = "lm", se = TRUE, color = "blue") + theme_minimal() + ggtitle("horsepower vs price")
age_plot <- ggplot(clean_age, aes(x = age, y = price)) + geom_point() + geom_smooth(method = "lm", se = TRUE, color = "blue") + theme_minimal() + ggtitle("age vs price")
ec_plot <- ggplot(clean_ec, aes(x = engine_capacity, y = price)) + geom_point() + geom_smooth(method = "lm", se = TRUE, color = "blue") + theme_minimal() + ggtitle("engine_capacity vs price")
grid.arrange(cm_plot,cm_plot2, hp_plot, age_plot, ec_plot, ncol=3)
```

[TODO: yap about graphs just like in slides]

Now moving on to the categorical variables.

We need to make sure that there's no correlation between the different categorical variables.

```{r echo=FALSE}
# Select numeric predictors only
numeric_data <- clean_data[sapply(clean_data, is.numeric)]

# Remove specific variables like 'Year'
numeric_data <- numeric_data[, !names(numeric_data) %in% c("year")]

# Compute correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Plot correlation matrix with ggcorrplot
ggcorrplot(
  cor_matrix, 
  method = "square",       # Style of the plot
  type = "lower",          # Show lower triangular correlation matrix
  lab = TRUE,              # Display correlation values inside the cells
  lab_size = 3,            # Adjust size of labels
  title = "Correlation Heatmap",  # Add a title
  colors = c("blue", "white", "red") # Define color gradient (negative, neutral, positive correlations)
)
```

Before we delve further into data analysis, we notice that there's some information in our dataset that is unlikely to be relevant, such as how many views or favourites the car posting gets, or the date of which it was posted. However, we need to run a t-test to make sure that those variables indeed do not have any influence on the final price of the car.

[TO DO: - get rid of certain variables like Views etc., justify using math/stats (can't just say "pretty sure it won't affect anything") - pretty sure it's just a basic beta_i = 0 t-test? correct me if I'm wrong]

## Categorical Variable Analysis

```{r echo=False, warning=FALSE}
# List of all categorical variables
categorical_vars <- c("post_info", "A_C", "emission_class", "seats_amount", 
                      "color", "type_of_drive", "doors", "fuel", 
                      "car_type", "gearbox")

# Create individual plots for each categorical variable
for (var in categorical_vars) {
  
  # Filter non-empty data for the current variable
  non_empty_data <- clean_data[!is.na(clean_data[[var]]) & !is.na(clean_data$price), ]

  # Check if the variable is a factor; if not, convert it
  if (!is.factor(non_empty_data[[var]])) {
    non_empty_data[[var]] <- as.factor(non_empty_data[[var]])
  }
  
  # Create the violin + box plot
  plot <- ggplot(non_empty_data, aes(x = .data[[var]], y = price)) +
    geom_violin(fill = "blue", alpha = 0.5) +
    geom_boxplot(width = 0.1, fill = "white", outlier.size = 0.5) +
    labs(title = paste("Price vs", var), y = "Price", x = "") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(plot)
}


```

## Model

```{r echo=False, warning=FALSE}
# List of all the variables to include in the model
categorical_vars <- c("post_info", "A_C", "emission_class", "seats_amount", 
                      "color", "type_of_drive", "doors", "fuel", 
                      "car_type", "gearbox")

# Convert categorical variables to factors
clean_data[categorical_vars] <- lapply(clean_data[categorical_vars], as.factor)

# Check if columns like car_name are factors or should be treated as character (if needed)
# Here we assume "car_name" is a unique identifier, so it should be excluded from the model
clean_data$car_name <- as.factor(clean_data$car_name)

# Fit the full linear model (including all variables except car_name as a unique identifier)
full_model <- lm(price ~ views + favorite + post_info + year + A_C + emission_class +
                 seats_amount + horsepower + color + car_mileage + engine_capacity + 
                 type_of_drive + doors + fuel + car_type + gearbox, data = clean_data)

# Check for multicollinearity (VIF - Variance Inflation Factor)
vif(full_model)

# Diagnostics: Check residuals for any patterns
par(mfrow = c(2, 2))  # Multiple plots in one window
plot(full_model)

step = stepAIC(full_model, trace = FALSE, direction = "backward")
step$anova

final_model = lm(price ~ views + favorite + post_info + year + A_C + emission_class +
                   seats_amount + horsepower + engine_capacity + type_of_drive + fuel + 
                   car_type + gearbox, data = clean_data)
```

## Model 2

```{r echo=FALSE, warning=FALSE}
# List of all the variables to include in the model
categorical_vars <- c("post_info", "A_C", "emission_class", "seats_amount", 
                      "color", "type_of_drive", "doors", "fuel", 
                      "car_type", "gearbox")

# Convert categorical variables to factors
clean_data[categorical_vars] <- lapply(clean_data[categorical_vars], as.factor)

# Check if columns like car_name are factors or should be treated as character (if needed)
# Here we assume "car_name" is a unique identifier, so it should be excluded from the model
clean_data$car_name <- as.factor(clean_data$car_name)

# Fit the linear model manually specifying all main effects and interaction terms
full_model <- lm(price ~ 
  age + car_mileage + engine_capacity + horsepower +  # Continuous variables
  post_info + A_C + seats_amount + color + type_of_drive + doors + fuel + car_type + gearbox + # Categorical variables
  age:post_info + age:A_C + age:seats_amount + age:color + age:type_of_drive + 
  age:doors + age:fuel + age:car_type + age:gearbox + 
  car_mileage:post_info + car_mileage:A_C + car_mileage:seats_amount + car_mileage:color + 
  car_mileage:type_of_drive + car_mileage:doors + car_mileage:fuel + car_mileage:car_type + car_mileage:gearbox + 
  engine_capacity:post_info + engine_capacity:A_C + engine_capacity:seats_amount + engine_capacity:color + 
  engine_capacity:type_of_drive + engine_capacity:doors + engine_capacity:fuel + engine_capacity:car_type + engine_capacity:gearbox + 
  horsepower:post_info + horsepower:A_C + horsepower:seats_amount + horsepower:color + 
  horsepower:type_of_drive + horsepower:doors + horsepower:fuel + horsepower:car_type + horsepower:gearbox, 
  data = clean_data)
print(vif(final_model))

# Diagnostics: Check residuals for any patterns
par(mfrow = c(2, 2))  # Multiple plots in one window
plot(full_model)

step = stepAIC(full_model, trace = FALSE, direction = "backward")
step$anova
step$call

final_model <- lm(formula = price ~ age + car_mileage + engine_capacity + horsepower + 
     post_info + A_C + seats_amount + color + type_of_drive + 
     doors + fuel + gearbox + age:A_C + age:seats_amount + 
     age:color + age:type_of_drive + age:doors + age:fuel + 
     age:gearbox + car_mileage:post_info + car_mileage:A_C + car_mileage:seats_amount + 
     car_mileage:type_of_drive + car_mileage:doors + car_mileage:fuel + 
     car_mileage:gearbox + engine_capacity:post_info + 
     engine_capacity:A_C + engine_capacity:seats_amount + engine_capacity:type_of_drive + 
     engine_capacity:fuel + engine_capacity:gearbox + 
     horsepower:post_info + horsepower:seats_amount + horsepower:color + 
     horsepower:fuel + horsepower:gearbox, 
     data = clean_data)

```

Diagnostics:

```{r warning=FALSE, echo=FALSE}
# Box-Cox transformation for `price`
boxcox_result <- boxcox(final_model, lambda = seq(-2, 2, by = 0.1), main = "Box-Cox Transformation")

# Extract the best lambda (transformation parameter)
best_lambda <- boxcox_result$x[which.max(boxcox_result$y)]
cat("Best lambda:", best_lambda, "\n")

if (abs(best_lambda) < 0.1) {
  clean_data$price <- log(clean_data$price)
  cat("Applied log transformation to `price`.\n")
} else {
  clean_data$price <- (clean_data$price^best_lambda - 1) / best_lambda
  cat("Applied Box-Cox transformation with lambda =", best_lambda, "to `price`.\n")
}

n <- nrow(clean_data)
cooks_d <- cooks.distance(final_model)
cooks_threshold <- 4/n #might need to change
influential_points <- which(cooks_d > cooks_threshold)
clean_data <- clean_data[-which(cooks_d > cooks_threshold), ]

final_model <- lm(formula = price ~ age + car_mileage + engine_capacity + horsepower + 
                     post_info + A_C + seats_amount + color + type_of_drive + 
                     doors + fuel + gearbox + age:A_C + age:seats_amount + 
                     age:color + age:type_of_drive + age:doors + age:fuel + 
                     age:gearbox + car_mileage:post_info + car_mileage:A_C + car_mileage:seats_amount + 
                     car_mileage:type_of_drive + car_mileage:doors + car_mileage:fuel + 
                     car_mileage:gearbox + engine_capacity:post_info + 
                     engine_capacity:A_C + engine_capacity:seats_amount + engine_capacity:type_of_drive + 
                     engine_capacity:fuel + engine_capacity:gearbox + 
                     horsepower:post_info + horsepower:seats_amount + horsepower:color + 
                     horsepower:fuel + horsepower:gearbox, 
                   data = clean_data)
par(mfrow = c(2, 2))
plot(final_model)


hist(resid(final_model), breaks = 30, main = "Histogram of Residuals", 
     xlab = "Residuals", col = "lightblue")

qqnorm(resid(final_model), main = "Q-Q Plot")
qqline(resid(final_model), col = "red", lty = 2)

# Perform Breusch-Pagan test
bp_test <- bptest(final_model)
print(bp_test)

# Interpretation
if (bp_test$p.value < 0.05) {
  cat("Heteroscedasticity detected (p-value < 0.05). Consider WLS.\n")
} else {
  cat("No significant heteroscedasticity detected.\n")
}

summary(final_model)
```

Using our hold-out validation set, we can perform a little bit more verification:

```{r echo=FALSE, warning=FALSE}
# validation data
validation_data <- car_price_data
validation_data[validation_data == ""] <- NA
sum(is.na(validation_data$emission_class))  # Number of rows with missing emission_class
validation_data <- validation_data[is.na(validation_data$emission_class), ]
validation_data <- validation_data[(validation_data$color != 'pink'), ]

validation_data$age <- 2024 - validation_data$year
validation_data$horsepower<-gsub(pattern = "^(\\d+) HP.*", replacement = "\\1", validation_data$horsepower)
validation_data$horsepower<-as.numeric(validation_data$horsepower)

names(validation_data) <- gsub(pattern = "\\.\\..*", replacement = "", names(validation_data))

# List of all categorical variables
categorical_vars <- c("post_info", "A_C", "emission_class", "seats_amount", 
                      "color", "type_of_drive", "doors", "fuel", 
                      "car_type", "gearbox")

for (var in categorical_vars) {
  if (!is.factor(validation_data[[var]])) {
      validation_data[[var]] <- as.factor(validation_data[[var]])
  }
}
validation_data <- validation_data[, names(validation_data) != "emission_class"]

# Specify the columns to exclude
#columns_to_exclude <- c("car_mileage", "column_name2", "column_name3")

# Exclude these columns from validation_data
#validation_data <- validation_data[, !(names(validation_data) %in% columns_to_exclude)]

# Step 1: Obtain predicted values for the validation set
model_without_emission <- lm(update(formula(final_model), . ~ . - emission_class), data = clean_data)
pred_validation <- predict(model_without_emission, newdata = validation_data)
train_validation <- predict(model_without_emission, newdata = clean_data)

# Step 2: Compute prediction errors
# Assuming 'price' is the dependent variable in validation_data
delta_validation <- validation_data$price - pred_validation
delta2_validation <- clean_data$price - train_validation
delta_validation <- na.omit(delta_validation)  # Remove missing errors
delta2_validation <- na.omit(delta2_validation)  # Remove missing errors

# Step 3: Compute MSPE
MSPE <- sum(delta_validation^2) / length(delta_validation)
MSPE2 <- sum(delta2_validation^2) / length(delta2_validation)

# Step 4: Print MSPE
MSPE
MSPE2

```

WLS is a next step since we failed homoscedasticity (equal variance) assumption. This means that we'd include a weight vector when fitting the model, and hopefully this would fix the equal variance problem.

[TODO: future improvements/limitations of model]
